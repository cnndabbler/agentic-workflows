{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnndabbler/agentic-workflows/blob/main/run_pydantic_ai_with_Ollama_and_DeepSeek_R1_Step_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "## Pydantic Agents running DeepSeek-R1 using Ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic_ai\n",
        "!pip install -q pydantic\n",
        "!pip install -q rich"
      ],
      "metadata": {
        "id": "2oodiS71x0Jy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate Ollama and load the models"
      ],
      "metadata": {
        "id": "bCpdkKPzc4X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Run the Ollama installation script\n",
        "subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, check=True)\n",
        "\n",
        "# Start the Ollama server in the background\n",
        "ollama_process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# Allow some time for the server to initialize\n",
        "time.sleep(5)\n",
        "\n",
        "# Reasoner models\n",
        "# !ollama pull deepseek-r1:32b > /dev/null 2>&1\n",
        "# !ollama pull deepseek-r1:32\n",
        "# !ollama pull deepseek-r1:14b > /dev/null 2>&1\n",
        "!ollama pull deepseek-r1:14b\n",
        "\n",
        "# Parser models\n",
        "# !ollama pull granite3.1-dense > /dev/null 2>&1\n",
        "!ollama pull granite3.1-dense\n",
        "\n"
      ],
      "metadata": {
        "id": "zSnwmQcqizeM",
        "outputId": "738edee0-549c-4198-d065-06beecf58425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 6e9f90f02bb3... 100% ▕▏ 9.0 GB                         \u001b[K\n",
            "pulling 369ca498f347... 100% ▕▏  387 B                         \u001b[K\n",
            "pulling 6e4c38e1172f... 100% ▕▏ 1.1 KB                         \u001b[K\n",
            "pulling f4d24e9138dd... 100% ▕▏  148 B                         \u001b[K\n",
            "pulling 3c24b0c80794... 100% ▕▏  488 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 44d19d212d76... 100% ▕▏ 5.0 GB                         \u001b[K\n",
            "pulling f76a906816c4... 100% ▕▏ 1.4 KB                         \u001b[K\n",
            "pulling f7b956e70ca3... 100% ▕▏   69 B                         \u001b[K\n",
            "pulling 492069a62c25... 100% ▕▏  11 KB                         \u001b[K\n",
            "pulling f9cd69f4077d... 100% ▕▏  491 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "7uns0GaLXKvR",
        "outputId": "f93cea9b-29b4-449a-e7f0-bb9058e95369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                       ID              SIZE      MODIFIED               \n",
            "granite3.1-dense:latest    34d3be74ec54    5.0 GB    Less than a second ago    \n",
            "deepseek-r1:14b            ea35dfe18182    9.0 GB    1 second ago              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Ollama Models for Pydantic Agents"
      ],
      "metadata": {
        "id": "vZ_fW4y-S4oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "\n",
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "\n",
        "# Ollama with DeepSeekR1\n",
        "deepseek_reasoner_model = OpenAIModel(\n",
        "    # 'deepseek-r1:32b',\n",
        "    'deepseek-r1:14b',\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"whatever\",\n",
        ")\n",
        "\n",
        "# Ollama with Granite\n",
        "granite_parser_model = OpenAIModel(\n",
        "    # 'qwen2.5',\n",
        "    'granite3.1-dense',\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"whatever\",\n",
        ")\n",
        "\n",
        "# Gemini\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Google Colab userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set the API key as an environment variable, which `pydantic-ai` will automatically use for Gemini\n",
        "os.environ['GEMINI_API_KEY'] = api_key\n",
        "\n",
        "gemini_parser_model = \"google-gla:gemini-1.5-flash\""
      ],
      "metadata": {
        "id": "jFGiFqK2fkSJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic Classes"
      ],
      "metadata": {
        "id": "SqU7mlU5iAW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field, confloat\n",
        "import asyncio\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "# Define the Joke model\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(..., description=\"The setup part of the joke\")\n",
        "    punchline: str = Field(..., description=\"The punchline of the joke\")\n",
        "    category: str = Field(..., description=\"Category of the joke (e.g., 'pun', 'wordplay', 'dad joke')\")\n",
        "    thinking: str = Field(..., description=\"the thought process and planning captured between <think> tags without any change\")\n"
      ],
      "metadata": {
        "id": "IEridau5f_ja"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Joke Creator Agent\n",
        "joke_creator = Agent(\n",
        "    model=deepseek_reasoner_model,\n",
        "    result_type=str,\n",
        "    deps_type=str,  # Takes a string prompt as input\n",
        "    system_prompt=(\n",
        "        \"You are a joke creator that specializes in creating funny, family-friendly dad jokes. \"\n",
        "        \"Your output MUST follow this EXACT format with EXACT field names:\\n\\n\"\n",
        "        \"Setup: [setup text]\\n\"\n",
        "        \"Punchline: [punchline text]\\n\"\n",
        "        \"Category: [category]\\n\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "48zghcKygXJR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binding to Pydantic Joke class: Failure\n",
        "\n",
        "- You will see the following error:\n",
        "- BadRequestError: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:32b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}"
      ],
      "metadata": {
        "id": "a7zl3dzxg8Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Joke Creator Agent binding result_type to Joke class\n",
        "joke_creator_with_pydantic_class = Agent(\n",
        "    model=deepseek_reasoner_model,\n",
        "    result_type=Joke,  # We'll attempt to parse into our Pydantic Joke model\n",
        "    deps_type=str,  # Takes a string prompt as input\n",
        "    system_prompt=(\n",
        "        \"You are a joke creator that specializes in creating funny, family-friendly dad jokes. \"\n",
        "        \"Your output MUST follow this EXACT format with EXACT field names:\\n\\n\"\n",
        "        \"Setup: [setup text]\\n\"\n",
        "        \"Punchline: [punchline text]\\n\"\n",
        "        \"Category: [category]\\n\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Akl5LYdEfsD4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Only run the cell below to verify that Pydantic Agent does not support DeepSeek-R1"
      ],
      "metadata": {
        "id": "Ww2tiWVJb5dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# joke_text = await joke_creator_with_pydantic_class.run(\"Create a dad joke\")\n",
        "# print(\"\\nRaw joke text:\")\n",
        "# print(\"-\" * 50)\n",
        "# print(joke_text.data)\n",
        "# print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "CSWsVKwuhLsB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Create Joke with DeepSeek-R1"
      ],
      "metadata": {
        "id": "eAHiaJTXgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create raw joke text using DeepSeek-R1\n",
        "print(\"Creating joke...\")\n",
        "joke_text = await joke_creator.run(\n",
        "    \"Create a dad joke\",\n",
        "    model_settings={\"max_tokens\": 2000}\n",
        "    )\n",
        "if not joke_text:\n",
        "    raise ValueError(\"Failed to create joke\")\n",
        "print(\"\\nRaw joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(joke_text.data)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "jT940VvGkh6_",
        "outputId": "1e0e2c06-f621-49c2-d278-f2cb6381d4fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating joke...\n",
            "\n",
            "Raw joke text:\n",
            "--------------------------------------------------\n",
            "<think>\n",
            "Okay, so I need to create a joke that's both funny and family-friendly for dads. First, I should think about common themes in dad jokes. Animals are always a safe and funny topic because they're relatable and easy to make puns with.\n",
            "\n",
            "I remember some animal-related setups, but I want something fresh. Maybe using an animal activity as the setup. Let me consider different animals: dogs, cats, birds... Oh, how about a penguin? Penguins do waddle, which is inherently funny.\n",
            "\n",
            "The waddling could lead to a Dad joke about walking or falling over, maybe in a grocery store. That scenario is amusing because it's unexpected—penguins don't usually stroll through grocery stores!\n",
            "\n",
            "Now for the punchline. It should tie the waddling back to the grocery shopping scenario with a humorous twist. Something like asking about balance would be perfect because penguins might not be too graceful on land, and dads can relate to that joke.\n",
            "\n",
            "Putting it all together:\n",
            "\n",
            "Setup: Why did the penguin get invited to the human's house?\n",
            "Punchline: Because they heard he was great at grocery store waddles… and could always find the balance!\n",
            "\n",
            "This works because it's simple, has a pun, is clear, and family-friendly. It's easy to understand and gives dad a light-hearted moment.\n",
            "</think>\n",
            "\n",
            "Setup: Why did the penguin get invited to the human's house?  \n",
            "Punchline: Because they heard he was great at grocery store waddles… and could always find the balance!  \n",
            "Category: Animal\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2A : Parser using Gemini"
      ],
      "metadata": {
        "id": "aS36Aij5cj4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Parser Agent with 3 retries max\n",
        "joke_parser_gemini = Agent(\n",
        "    model=gemini_parser_model,\n",
        "    result_type=Joke,  # We'll parse into our Pydantic Joke model\n",
        "    retries=3, # Maximum retries\n",
        "    # deps_type=str,\n",
        "    system_prompt=(\n",
        "        \"You are a joke parser that extracts components from the input text to create a Joke object. \"\n",
        "        \"Your ONLY task is to find and extract these components from the input text without making any change:\\n\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Parse the text into a Pydantic model using Gemini\n",
        "print(\"\\nParsing joke...\")\n",
        "parsed_joke = await joke_parser_gemini.run(\n",
        "    joke_text.data)\n",
        "if not parsed_joke:\n",
        "    raise ValueError(\"Failed to parse joke\")\n",
        "print(\"\\nParsed joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(parsed_joke.data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Step 3: Display results\n",
        "console = Console()\n",
        "\n",
        "# Create assessment table\n",
        "table = Table(title=\"Joke Parsed\", box=box.ROUNDED)\n",
        "table.add_column(\"Property\", style=\"cyan\", width=20)\n",
        "table.add_column(\"Value\", style=\"yellow\")\n",
        "\n",
        "# Add joke content\n",
        "table.add_row(\"Setup\", parsed_joke.data.setup)\n",
        "table.add_row(\"Punchline\", parsed_joke.data.punchline)\n",
        "table.add_row(\"Category\", parsed_joke.data.category)\n",
        "table.add_row(\"\",\"\")\n",
        "\n",
        "\n",
        "# Add thinking process\n",
        "table.add_row(\"\", \"\")\n",
        "table.add_row(\"[bold]Thinking Process\", \"\")\n",
        "table.add_row(\"\", parsed_joke.data.thinking)\n",
        "\n",
        "# Print the table\n",
        "console.print(\"\\n\")\n",
        "console.print(table)"
      ],
      "metadata": {
        "id": "B53ctQD0chZX",
        "outputId": "1a39ddb8-3e8e-4658-c389-e7e02d325908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parsing joke...\n",
            "\n",
            "Parsed joke text:\n",
            "--------------------------------------------------\n",
            "setup=\"Why did the penguin get invited to the human's house?\" punchline='Because they heard he was great at grocery store waddles… and could always find the balance!' category='Animal' thinking=\"\\\\nOkay, so I need to create a joke that's both funny and family-friendly for dads. First, I should think about common themes in dad jokes. Animals are always a safe and funny topic because they're relatable and easy to make puns with.\\\\n\\\\nI remember some animal-related setups, but I want something fresh. Maybe using an animal activity as the setup. Let me consider different animals: dogs, cats, birds... Oh, how about a penguin? Penguins do waddle, which is inherently funny.\\\\n\\\\nThe waddling could lead to a Dad joke about walking or falling over, maybe in a grocery store. That scenario is amusing because it's unexpected—penguins don't usually stroll through grocery stores!\\\\n\\\\nNow for the punchline. It should tie the waddling back to the grocery shopping scenario with a humorous twist. Something like asking about balance would be perfect because penguins might not be too graceful on land, and dads can relate to that joke.\\\\n\\\\nPutting it all together:\\\\n\\\\nSetup: Why did the penguin get invited to the human's house?\\\\nPunchline: Because they heard he was great at grocery store waddles… and could always find the balance!\\\\n\\\\nThis works because it's simple, has a pun, is clear, and family-friendly. It's easy to understand and gives dad a light-hearted moment.\\\\n\"\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                                    Joke Parsed                                                    \u001b[0m\n",
              "╭──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│\u001b[1m \u001b[0m\u001b[1mProperty            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                                                   \u001b[0m\u001b[1m \u001b[0m│\n",
              "├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│\u001b[36m \u001b[0m\u001b[36mSetup               \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mWhy did the penguin get invited to the human's house?                                   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mPunchline           \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mBecause they heard he was great at grocery store waddles… and could always find the     \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mbalance!                                                                                \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mCategory            \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mAnimal                                                                                  \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[1;36mThinking Process\u001b[0m\u001b[36m    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\\nOkay, so I need to create a joke that's both funny and family-friendly for dads.      \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mFirst, I should think about common themes in dad jokes. Animals are always a safe and   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mfunny topic because they're relatable and easy to make puns with.\\n\\nI remember some    \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33manimal-related setups, but I want something fresh. Maybe using an animal activity as the\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33msetup. Let me consider different animals: dogs, cats, birds... Oh, how about a penguin? \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mPenguins do waddle, which is inherently funny.\\n\\nThe waddling could lead to a Dad joke \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mabout walking or falling over, maybe in a grocery store. That scenario is amusing       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mbecause it's unexpected—penguins don't usually stroll through grocery stores!\\n\\nNow for\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mthe punchline. It should tie the waddling back to the grocery shopping scenario with a  \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mhumorous twist. Something like asking about balance would be perfect because penguins   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mmight not be too graceful on land, and dads can relate to that joke.\\n\\nPutting it all  \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mtogether:\\n\\nSetup: Why did the penguin get invited to the human's house?\\nPunchline:   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mBecause they heard he was great at grocery store waddles… and could always find the     \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mbalance!\\n\\nThis works because it's simple, has a pun, is clear, and family-friendly.   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mIt's easy to understand and gives dad a light-hearted moment.\\n                         \u001b[0m\u001b[33m \u001b[0m│\n",
              "╰──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Joke Parsed                                                    </span>\n",
              "╭──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│<span style=\"font-weight: bold\"> Property             </span>│<span style=\"font-weight: bold\"> Value                                                                                    </span>│\n",
              "├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Setup                </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Why did the penguin get invited to the human's house?                                    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Punchline            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Because they heard he was great at grocery store waddles… and could always find the      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> balance!                                                                                 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Category             </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Animal                                                                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Thinking Process</span><span style=\"color: #008080; text-decoration-color: #008080\">     </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \\nOkay, so I need to create a joke that's both funny and family-friendly for dads.       </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> First, I should think about common themes in dad jokes. Animals are always a safe and    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> funny topic because they're relatable and easy to make puns with.\\n\\nI remember some     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> animal-related setups, but I want something fresh. Maybe using an animal activity as the </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> setup. Let me consider different animals: dogs, cats, birds... Oh, how about a penguin?  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Penguins do waddle, which is inherently funny.\\n\\nThe waddling could lead to a Dad joke  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> about walking or falling over, maybe in a grocery store. That scenario is amusing        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> because it's unexpected—penguins don't usually stroll through grocery stores!\\n\\nNow for </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> the punchline. It should tie the waddling back to the grocery shopping scenario with a   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> humorous twist. Something like asking about balance would be perfect because penguins    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> might not be too graceful on land, and dads can relate to that joke.\\n\\nPutting it all   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> together:\\n\\nSetup: Why did the penguin get invited to the human's house?\\nPunchline:    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Because they heard he was great at grocery store waddles… and could always find the      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> balance!\\n\\nThis works because it's simple, has a pun, is clear, and family-friendly.    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> It's easy to understand and gives dad a light-hearted moment.\\n                          </span>│\n",
              "╰──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2B: Ollama with Granite Model Parser"
      ],
      "metadata": {
        "id": "ioYHHXQfdOaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Parser Agent with 10 retries max\n",
        "joke_parser_ollama = Agent(\n",
        "    model=granite_parser_model,\n",
        "    result_type=Joke,  # We'll parse into our Pydantic Joke model\n",
        "    retries=10,\n",
        "    # deps_type=str,\n",
        "    system_prompt=(\n",
        "        \"You are a joke parser that extracts components from the input text to create a Joke object. \"\n",
        "        \"Your ONLY task is to find and extract these components from the input text without making any change:\\n\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Parse the text into a Pydantic model using Ollama Granite\n",
        "print(\"\\nParsing joke...\")\n",
        "parsed_joke = await joke_parser_ollama.run(\n",
        "    joke_text.data)\n",
        "if not parsed_joke:\n",
        "    raise ValueError(\"Failed to parse joke\")\n",
        "print(\"\\nParsed joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(parsed_joke.data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Step 3: Display results\n",
        "console = Console()\n",
        "\n",
        "# Create assessment table\n",
        "table = Table(title=\"Joke Parsed\", box=box.ROUNDED)\n",
        "table.add_column(\"Property\", style=\"cyan\", width=20)\n",
        "table.add_column(\"Value\", style=\"yellow\")\n",
        "\n",
        "# Add joke content\n",
        "table.add_row(\"Setup\", parsed_joke.data.setup)\n",
        "table.add_row(\"Punchline\", parsed_joke.data.punchline)\n",
        "table.add_row(\"Category\", parsed_joke.data.category)\n",
        "table.add_row(\"\",\"\")\n",
        "\n",
        "\n",
        "# Add thinking process\n",
        "table.add_row(\"\", \"\")\n",
        "table.add_row(\"[bold]Thinking Process\", \"\")\n",
        "table.add_row(\"\", parsed_joke.data.thinking)\n",
        "\n",
        "# Print the table\n",
        "console.print(\"\\n\")\n",
        "console.print(table)"
      ],
      "metadata": {
        "id": "DMc379Ea9fS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "58a50b3f-19f1-4839-b55a-68ca0d4c6b1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parsing joke...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnexpectedModelBehavior",
          "evalue": "Exceeded maximum retries (10) for result validation",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnexpectedModelBehavior\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-93fb872b850f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Parse the text into a Pydantic model using Ollama Granite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nParsing joke...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m parsed_joke = await joke_parser_ollama.run(\n\u001b[0m\u001b[1;32m     16\u001b[0m     joke_text.data)\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_joke\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_prompt, result_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0musage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         ) as agent_run:\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36m__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     ):\n\u001b[1;32m   1297\u001b[0m         \u001b[0;34m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__anext__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     async def next(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_graph/graph.py\u001b[0m in \u001b[0;36m__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_graph/graph.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_graph/graph.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, node, history, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mstart_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow_utc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGraphRunContext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphAgentState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphAgentDeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDepsT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNodeRunEndT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     ) -> Union[ModelRequestNode[DepsT, NodeRunEndT], End[result.FinalResult[NodeRunEndT]]]:  # noqa UP007\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0manext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# Run the stream to completion if it was not finished:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_event\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36m_run_stream\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36m_run_stream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;31m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_text_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedModelBehavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Received empty model response'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36m_handle_text_response\u001b[0;34m(self, ctx, texts)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_final_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFinalResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_retries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_result_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             return ModelRequestNode[DepsT, NodeRunEndT](\n\u001b[1;32m    481\u001b[0m                 _messages.ModelRequest(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36mincrement_retries\u001b[0;34m(self, max_result_retries)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_result_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             raise exceptions.UnexpectedModelBehavior(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;34mf'Exceeded maximum retries ({max_result_retries}) for result validation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             )\n",
            "\u001b[0;31mUnexpectedModelBehavior\u001b[0m: Exceeded maximum retries (10) for result validation"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "name": "run-pydantic-ai-with-Ollama-and-DeepSeek-R1.ipynb",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}