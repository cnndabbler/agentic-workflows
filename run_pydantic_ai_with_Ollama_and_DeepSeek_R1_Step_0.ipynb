{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnndabbler/agentic-workflows/blob/main/run_pydantic_ai_with_Ollama_and_DeepSeek_R1_Step_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "## Pydantic Agents running DeepSeek-R1 using Ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic_ai\n",
        "!pip install -q pydantic\n",
        "!pip install -q rich"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oodiS71x0Jy",
        "outputId": "a8637f99-1574-4870-9b7c-c598eed8d06e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/223.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Run the Ollama installation script\n",
        "subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, check=True)\n",
        "\n",
        "# Start the Ollama server in the background\n",
        "ollama_process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# Reasoner model\n",
        "!ollama pull deepseek-r1:32b > /dev/null 2>&1\n",
        "# Parser and Judge models\n",
        "!ollama pull qwen2.5 > /dev/null 2>&1\n",
        "\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUhagysPHI1h",
        "outputId": "97b2ee7b-f163-4454-b930-df85e4c2ba24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME               ID              SIZE      MODIFIED               \n",
            "qwen2.5:latest     845dbda0ea48    4.7 GB    Less than a second ago    \n",
            "deepseek-r1:32b    38056bbcbb2d    19 GB     33 seconds ago            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama Models"
      ],
      "metadata": {
        "id": "vZ_fW4y-S4oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field, confloat\n",
        "import asyncio\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich import box"
      ],
      "metadata": {
        "id": "4g5kdVtvxUau"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "# reasoner_model = \"ollama:deepseek-r1:14b\"  # Main reasoning model\n",
        "# formatter_model = \"ollama:qwen2.5\"  # Formatter model\n",
        "\n",
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "\n",
        "# DeepSeekR1\n",
        "deepseek_reasoner_model = OpenAIModel(\n",
        "    'deepseek-r1:32b',\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"whatever\",\n",
        ")\n",
        "\n",
        "deepseek_parser_model = OpenAIModel(\n",
        "    'qwen2.5',\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"whatever\",\n",
        ")"
      ],
      "metadata": {
        "id": "jFGiFqK2fkSJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic Classes"
      ],
      "metadata": {
        "id": "SqU7mlU5iAW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field, confloat\n",
        "import asyncio\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "# Define the Joke model\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(..., description=\"The setup part of the joke\")\n",
        "    punchline: str = Field(..., description=\"The punchline of the joke\")\n",
        "    category: str = Field(..., description=\"Category of the joke (e.g., 'pun', 'wordplay', 'dad joke')\")\n",
        "    thinking: str = Field(..., description=\"the thought process and planning\")\n",
        "\n",
        "class JokeAssessment(BaseModel):\n",
        "    originality: confloat(ge=0, le=10) = Field(..., description=\"Rating of joke originality from 0-10\")\n",
        "    humor_level: confloat(ge=0, le=10) = Field(..., description=\"Rating of humor level from 0-10\")\n",
        "    family_friendly: bool = Field(..., description=\"Whether the joke is family-friendly\")\n",
        "    wordplay_quality: confloat(ge=0, le=10) = Field(..., description=\"Rating of wordplay quality from 0-10\")\n",
        "    delivery_structure: confloat(ge=0, le=10) = Field(..., description=\"Rating of setup-punchline structure from 0-10\")\n",
        "    strengths: List[str] = Field(..., description=\"List of joke's strong points\")\n",
        "    improvement_suggestions: List[str] = Field(..., description=\"Think step buy step and provide a very extensive List of very specific suggestions for improvement\")\n",
        "    overall_score: confloat(ge=0, le=10) = Field(..., description=\"Overall joke quality score from 0-10\")\n"
      ],
      "metadata": {
        "id": "IEridau5f_ja"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Joke Creator Agent\n",
        "joke_creator = Agent(\n",
        "    model=deepseek_reasoner_model,\n",
        "    result_type=str,\n",
        "    deps_type=str,  # Takes a string prompt as input\n",
        "    system_prompt=(\n",
        "        \"You are a joke creator that specializes in creating funny, family-friendly dad jokes. \"\n",
        "        \"Your output MUST follow this EXACT format with EXACT field names:\\n\\n\"\n",
        "        \"Thinking: \\n<think>\\n[your detailed thought process without changes]\\n</think>\\n\\n\"\n",
        "        \"Setup: [setup text]\\n\"\n",
        "        \"Punchline: [punchline text]\\n\"\n",
        "        \"Category: [category]\\n\"\n",
        "        # \"Thinking: [brief explanation]\\n\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "48zghcKygXJR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic AI with DeepSeek-R1: No class binding"
      ],
      "metadata": {
        "id": "VKUc7ewKftka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specifying the Joke class"
      ],
      "metadata": {
        "id": "j83kR2G1gl3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joke_text = await joke_creator.run(\"Create a dad joke\")\n",
        "print(\"\\nRaw joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(joke_text.data)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "fgPTNRr9f4Fp",
        "outputId": "4eb3f650-e10c-4722-8f92-887ec865b20c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw joke text:\n",
            "--------------------------------------------------\n",
            "<think>\n",
            "Alright, so the user wants me to create a dad joke. They've given specific formatting instructions with \"Thinking:\" and \"Create a dad joke\". I need to make sure the joke is family-friendly and fits their required structure.\n",
            "\n",
            "First, considering dad jokes are usually puns or plays on words. Let's brainstorm some everyday objects that can have double meanings. Maybe something related to school since it's relatable for families.\n",
            "\n",
            "\"Note\" comes to mind because it has multiple meanings – a written note, like in music or school assignments, and the idea of giving a kid notes they need to learn. It can also tie into a teacher’s perspective, adding a layered joke.\n",
            "\n",
            "So, using \"note\" as the core. The setup needs to engage and lead to the punchline smoothly. Maybe asking how much a teacher values their students? Then the punchline reveals it's by the number of notes they give, which is both about music notes and maybe detention notes. That adds a funny twist with a bit of mischief.\n",
            "\n",
            "I think this fits well because it’s simple, relatable, and has that dad joke humor without being too complex or risqué. Plus, the category as \"school/humor\" makes sense since it revolves around teachers and students.\n",
            "</think>\n",
            "\n",
            "Thinking: Let me come up with a Dad joke about something relatable but funny. Notes are used in music and school, which is perfect for a play on words. Teachers give notes to students or play notes on instruments. It’s simple and family-friendly.\n",
            "\n",
            "Setup: Why did the teacher value her students?  \n",
            "Punchline: Because she graded them by the number of *notes* they played!  \n",
            "Category: School/humor\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binding to Pydantic Joke class: Failure\n",
        "\n",
        "- You will see the following error:\n",
        "- BadRequestError: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:32b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}"
      ],
      "metadata": {
        "id": "a7zl3dzxg8Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Joke Creator Agent\n",
        "joke_creator_with_pydantic_class = Agent(\n",
        "    model=deepseek_reasoner_model,\n",
        "    result_type=Joke,\n",
        "    deps_type=str,  # Takes a string prompt as input\n",
        "    system_prompt=(\n",
        "        \"You are a joke creator that specializes in creating funny, family-friendly dad jokes. \"\n",
        "        \"Your output MUST follow this EXACT format with EXACT field names:\\n\\n\"\n",
        "        \"Thinking: \\n<think>\\n[your detailed thought process without changes]\\n</think>\\n\\n\"\n",
        "        \"Setup: [setup text]\\n\"\n",
        "        \"Punchline: [punchline text]\\n\"\n",
        "        \"Category: [category]\\n\"\n",
        "        # \"Thinking: [brief explanation]\\n\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Akl5LYdEfsD4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_text = await joke_creator_with_pydantic_class.run(\"Create a dad joke\")\n",
        "print(\"\\nRaw joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(joke_text.data)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "CSWsVKwuhLsB",
        "outputId": "5456154a-463d-4a88-9925-711c97f2724d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:32b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-964d46cd8f78>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoke_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mjoke_creator_with_pydantic_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create a dad joke\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRaw joke text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoke_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# Actually run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             end_result, _ = await graph.run(\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_graph/graph.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, start_node, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_graph/graph.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, node, history, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mstart_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow_utc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/_agent_graph.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mmodel_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_model_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_logfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model request'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             model_response, request_usage = await ctx.deps.model.request(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_request_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/models/openai.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, messages, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    146\u001b[0m     ) -> tuple[ModelResponse, usage.Usage]:\n\u001b[1;32m    147\u001b[0m         \u001b[0mcheck_allow_model_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         response = await self._completions_create(\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOpenAIModelSettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_request_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/models/openai.py\u001b[0m in \u001b[0;36m_completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopenai_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         return await self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1726\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         )\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         return await self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:32b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a Parser Agent to successfully bind to Pydantic Class"
      ],
      "metadata": {
        "id": "eAHiaJTXgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Parser Agent\n",
        "joke_parser = Agent(\n",
        "    model=deepseek_parser_model,\n",
        "    result_type=Joke,\n",
        "    deps_type=str,  # Takes raw text as input\n",
        "    system_prompt=(\n",
        "        \"You are a joke parser that extracts components from the input text to create a Joke object. \"\n",
        "        \"Your ONLY task is to find and extract these components from the input text:\\n\"\n",
        "        \"1. The setup (after 'Setup:')\\n\"\n",
        "        \"2. The punchline (after 'Punchline:')\\n\"\n",
        "        \"3. The category (after 'Category:')\\n\"\n",
        "        \"4. The thinking (between <think> tags)\\n\\n\"\n",
        "        \"Return the components in this format:\\n\"\n",
        "        \"setup=[text after Setup:]\\n\"\n",
        "        \"punchline=[text after Punchline:]\\n\"\n",
        "        \"category=[text after Category:]\\n\"\n",
        "        \"thinking=[text between <think> tags]\\n\\n\"\n",
        "        \"DO NOT generate new content or modify the text in any way.\\n\"\n",
        "        \"DO NOT include any other text in your response.\"\n",
        "    ),\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "DMc379Ea9fS2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create raw joke text using DeepSeek-R1\n",
        "print(\"Creating joke...\")\n",
        "joke_text = await joke_creator.run(\"Create a dad joke\")\n",
        "if not joke_text:\n",
        "    raise ValueError(\"Failed to create joke\")\n",
        "print(\"\\nRaw joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(joke_text.data)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-X-9mBB1XJu",
        "outputId": "7ddbc929-faff-4616-d588-65d3ed6295fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating joke...\n",
            "\n",
            "Raw joke text:\n",
            "--------------------------------------------------\n",
            "<think>\n",
            "Alright, the user wants me to create a family-friendly dad joke. They specified that the output must follow an exact format with specific field names, including Thinking and a structured response. \n",
            "\n",
            "First, I need to think of a topic that's universally relatable but also allows for a pun. Dinosaurs are always a safe bet—they’re fun and not too adult-oriented.\n",
            "\n",
            "Next, I'll brainstorm what dinosaurs might do around the house or during free time. Maybe something they could \"dig\" into, which is both literal and a play on words. \n",
            "\n",
            "For the punchline, it should tie back to the setup in a surprising way that makes people chuckle without being offensive. Using \"rocking the Casbah\" adds a humorous twist because it's unexpected and ties into the idea of dinosaurs exploring different areas.\n",
            "\n",
            "Finally, I need to ensure the joke fits the family-friendly requirement. It’s light-hearted, doesn’t involve any adult themes, and is easy enough for kids to understand while still being funny for adults.\n",
            "</think>\n",
            "\n",
            "Thinking:  \n",
            "Alright, let's tackle this dad joke creation. The user wants something family-friendly, so I'll steer clear of anything too edgy or adult-oriented. I should focus on playful wordplay and puns that kids can enjoy while still making the parents chuckle.\n",
            "\n",
            "First, I need a relatable topic. Dinosaurs are always a hit with kids! They're fascinating and provide plenty of opportunities for humor. Maybe something related to how dinosaurs spend their time. \n",
            "\n",
            "Next, I'll brainstorm what a dinosaur might do in its spare time. \"Digging\" is a natural fit because it rhymes with \"dinosaur,\" making it easy to craft a pun. Now, the punchline should tie back to the setup but with a twist that makes people laugh.\n",
            "\n",
            "Putting it all together: Dinosaurs get their kicks digging for treasure or maybe even rocks (get it? \"rocking\" the Casbah). That gives a nice twist and keeps it light-hearted and fun!\n",
            "\n",
            "Setup: What do dinosaurs enjoy doing in their spare time?  \n",
            "Punchline: Rocking the Casbah—digging for treasure!  \n",
            "Category: Dinosaurs\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Parse the text into a Pydantic model using Qwen2.5\n",
        "print(\"\\nParsing joke...\")\n",
        "parsed_joke = await joke_parser.run(joke_text.data)\n",
        "if not parsed_joke:\n",
        "    raise ValueError(\"Failed to parse joke\")\n",
        "print(\"\\nParsed joke text:\")\n",
        "print(\"-\" * 50)\n",
        "print(parsed_joke.data)\n",
        "print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "-HxYYd3nnou5",
        "outputId": "7340ad9c-4b3f-4a2a-9490-704096c9379c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parsing joke...\n",
            "\n",
            "Parsed joke text:\n",
            "--------------------------------------------------\n",
            "setup='Why did the teacher value her students?' punchline='Because she graded them by the number of *notes* they played!' category='School/humor' thinking='Thinking: Let me come up with a Dad joke about something relatable but funny. Notes are used in music and school, which is perfect for a play on words. Teachers give notes to students or play notes on instruments. It’s simple and family-friendly.'\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Display results\n",
        "console = Console()\n",
        "\n",
        "# Create assessment table\n",
        "table = Table(title=\"Joke Parsed\", box=box.ROUNDED)\n",
        "table.add_column(\"Property\", style=\"cyan\", width=20)\n",
        "table.add_column(\"Value\", style=\"yellow\")\n",
        "\n",
        "# Add joke content\n",
        "table.add_row(\"Setup\", parsed_joke.data.setup)\n",
        "table.add_row(\"Punchline\", parsed_joke.data.punchline)\n",
        "table.add_row(\"Category\", parsed_joke.data.category)\n",
        "table.add_row(\"\",\"\")\n",
        "\n",
        "\n",
        "# Add thinking process\n",
        "table.add_row(\"\", \"\")\n",
        "table.add_row(\"[bold]Thinking Process\", \"\")\n",
        "thinking_lines = parsed_joke.data.thinking.split('\\n')\n",
        "for line in thinking_lines:\n",
        "    if line.strip():\n",
        "        table.add_row(\"\", line)\n",
        "\n",
        "# Print the table\n",
        "console.print(\"\\n\")\n",
        "console.print(table)"
      ],
      "metadata": {
        "id": "HFBWDE1AnhbY",
        "outputId": "33b81813-3a6a-48e4-9cc1-baab981e9dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                                    Joke Parsed                                                    \u001b[0m\n",
              "╭──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│\u001b[1m \u001b[0m\u001b[1mProperty            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                                                   \u001b[0m\u001b[1m \u001b[0m│\n",
              "├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│\u001b[36m \u001b[0m\u001b[36mSetup               \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mWhy did the teacher value her students?                                                 \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mPunchline           \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mBecause she graded them by the number of *notes* they played!                           \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mCategory            \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mSchool/humor                                                                            \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[1;36mThinking Process\u001b[0m\u001b[36m    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                                                                                        \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mThinking: Let me come up with a Dad joke about something relatable but funny. Notes are \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mused in music and school, which is perfect for a play on words. Teachers give notes to  \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m                      \u001b[0m│\u001b[33m \u001b[0m\u001b[33mstudents or play notes on instruments. It’s simple and family-friendly.                 \u001b[0m\u001b[33m \u001b[0m│\n",
              "╰──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Joke Parsed                                                    </span>\n",
              "╭──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│<span style=\"font-weight: bold\"> Property             </span>│<span style=\"font-weight: bold\"> Value                                                                                    </span>│\n",
              "├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Setup                </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Why did the teacher value her students?                                                  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Punchline            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Because she graded them by the number of *notes* they played!                            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> Category             </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> School/humor                                                                             </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Thinking Process</span><span style=\"color: #008080; text-decoration-color: #008080\">     </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                                                                          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> Thinking: Let me come up with a Dad joke about something relatable but funny. Notes are  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> used in music and school, which is perfect for a play on words. Teachers give notes to   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> students or play notes on instruments. It’s simple and family-friendly.                  </span>│\n",
              "╰──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "name": "run-pydantic-ai-with-Ollama-and-DeepSeek-R1.ipynb",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}